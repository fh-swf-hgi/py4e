
Visualisierung von Daten
========================

Bisher haben wir Python erlernt und uns damit beschäftigt, wie man Python, das Netzwerk und Datenbanken verwendet, um Daten zu manipulieren.

In diesem Kapitel werfen wir einen Blick auf drei vollständige Anwendungen, die all diese Dinge zusammenbringen, um Daten zu verwalten und zu visualisieren. Wir können diese Anwendungen als Beispielcode verwenden, um mit dem Lösen eines realen Problems zu beginnen.

Jede der Anwendungen liegt als ZIP-Datei vor, die wir herunterladen und auf unseren Computern entpacken und ausführen können.

Erstellen einer OpenStreetMap aus geokodierten Daten
-------------------------------------------

\index{Google!Map}
\index{OpenStreetMap}
\index{Visualisierung!Map}

In diesem Projekt verwenden wir die OpenStreetMap-API, um die Standorte einiger vom Benutzer eingegebenen Universitätsnamen auf einer aktuellen OpenStreetMap zu platzieren.

![Eine OpenStreetMap](../images/openstreet-map)

Um loszulegen, laden wir die Anwendung hier herunter:

[www.py4e.com/code3/opengeo.zip](http://www.py4e.com/code3/opengeo.zip)

Das erste zu lösende Problem ist, dass diese Geokodierungs-APIs auf eine bestimmte Anzahl von Anfragen pro Tag begrenzt sind. Wenn wir viele Daten haben, müssen wir den Lookup-Prozess möglicherweise mehrmals anhalten und neu starten. Also teilen wir das Problem in zwei Phasen auf.

\index{Cache}

In der ersten Phase nehmen wir die Daten (Universitätsnamen) in der Datei `where.data` und lesen sie zeilenweise ein, rufen die geokodierten Informationen von Google ab und speichern sie in einer Datenbank `geodata.sqlite`. Bevor wir die Geokodierungs-API für jeden vom Benutzer eingegebenen Ort verwenden, prüfen wir zunächst, ob wir die Daten für diese bestimmte Eingabezeile bereits haben. Die Datenbank fungiert als lokaler *Zwischenspeicher* (*Cache*) unserer Geokodierungsdaten, um sicherzustellen, dass wir Google nie zweimal nach denselben Daten fragen.

Wir können den Prozess jederzeit neu starten, indem wir die Datei `geodata.sqlite` löschen.

Nun sollte das Programm `geoload.py` ausgeführt werden. Dieses Programm liest die Eingabezeilen in `where.data` und prüft für jede Zeile, ob sie bereits in der Datenbank vorhanden ist. Wenn wir die Daten für den Ort nicht haben, wird es die Geokodierungs-API aufrufen, um die Daten abzurufen und in der Datenbank zu speichern.

Hier ist ein Beispiellauf, nachdem bereits einige Daten in der Datenbank vorhanden sind:

~~~~
Found in database AGH University of Science and Technology

Found in database Academy of Fine Arts Warsaw Poland

Found in database American University in Cairo

Found in database Arizona State University

Found in database Athens Information Technology

Retrieving https://py4e-data.dr-chuck.net/
   opengeo?q=BITS+Pilani
Retrieved 794 characters {"type":"FeatureColl

Retrieving https://py4e-data.dr-chuck.net/
   opengeo?q=Babcock+University
Retrieved 760 characters {"type":"FeatureColl

Retrieving https://py4e-data.dr-chuck.net/
   opengeo?q=Banaras+Hindu+University
Retrieved 866 characters {"type":"FeatureColl
...
~~~~

Die ersten fünf Orte sind bereits in der Datenbank und werden daher übersprungen. Das Programm scannt bis zu dem Punkt, an dem es neue Orte findet und beginnt, diese abzurufen.

Das Programm `geoload.py` kann jederzeit gestoppt werden und es gibt einen Zähler, mit dem wir die Anzahl der Aufrufe der Geokodierungs-API für jeden Lauf begrenzen können. In Anbetracht der Tatsache, dass die Datei `where.data` nur ein paar hundert Datenelemente enthält, sollten wir nicht an das tägliche Google-Zugriffslimit stoßen, aber wenn wir mehr Daten hätten, könnte es mehrere Durchläufe über mehrere Tage hinweg erfordern, bis unsere Datenbank alle geokodierten Daten für unsere Eingabe enthält.

Sobald wir einige Daten in `geodata.sqlite` geladen haben, können wir die Daten mit dem Programm `geodump.py` visualisieren. Dieses Programm liest die Datenbank und schreibt die Datei `where.js` mit dem Standort, dem Breitengrad und dem Längengrad in Form von ausführbarem JavaScript-Code.

Ein Durchlauf des Programms `geodump.py` sieht folgendermaßen aus:

~~~~
AGH University of Science and Technology, Czarnowiejska,
Czarna Wieś, Krowodrza, Kraków, Lesser Poland
Voivodeship, 31-126, Poland 50.0657 19.91895

Academy of Fine Arts, Krakowskie Przedmieście,
Northern Śródmieście, Śródmieście, Warsaw, Masovian
Voivodeship, 00-046, Poland 52.239 21.0155
...
260 lines were written to where.js
Open the where.html file in a web browser to view the data.
~~~~

Die Datei `where.html` besteht aus HTML und JavaScript zur Visualisierung einer Google-Karte. Sie liest die aktuellsten Daten aus `where.js`, um die zu visualisierenden Daten zu erhalten. Hier ist das Format der `where.js` Datei:

~~~~ {.js}
myData = [
[50.0657,19.91895,
'AGH University of Science and Technology, Czarnowiejska,
Czarna Wieś, Krowodrza, Kraków, Lesser Poland
Voivodeship, 31-126, Poland '],
[52.239,21.0155,
'Academy of Fine Arts, Krakowskie Przedmieściee,
Śródmieście Północne, Śródmieście, Warsaw,
Masovian Voivodeship, 00-046, Poland'],
   ...
];
~~~~

Dies ist eine JavaScript-Variable, die eine Liste von Listen enthält. Die Syntax für JavaScript-Listenkonstanten ist der von Python sehr ähnlich, daher sollte Ihnen die Syntax vertraut sein.

Nun muss die `where.html` in einem Browser geöffnet werden, um die Standorte zu sehen. Wir können den Mauszeiger über jede Pin bewegen, um den Standort zu finden, den die Geokodierungs-API für die vom Benutzer eingegebene Eingabe zurückgegeben hat. Wenn wir beim Öffnen der Datei `where.html` keine Daten sehen, sollten wir die JavaScript- oder Entwicklerkonsole für unseren Browser überprüfen.

Visualisierung von Netzwerken
-----------------------------

\index{Google!Page-Ranking}
\index{Visualisierung!Netzwerke}
\index{Visualisierung!Page-Ranking}

In dieser Anwendung werden wir einige der Funktionen einer Suchmaschine ausführen. Wir werden zunächst eine kleine Teilmenge des Webs durchforsten und eine vereinfachte Version des Google-Page-Ranking-Algorithmus ausführen, um festzustellen, welche Seiten am stärksten miteinander verbunden sind, und dann den Seitenrang und die Verbindung in unserer kleinen Ecke des Webs visualisieren. Wir werden die D3-JavaScript-Visualisierungsbibliothek <http://d3js.org/> verwenden, um die Visualisierungsausgabe zu erzeugen.

Wir können diese Anwendung hier herunterladen:

[www.py4e.com/code3/pagerank.zip](http://www.py4e.com/code3/pagerank.zip)

![Page-Ranking](height=3.5in@../images/pagerank)

Das erste Programm (`spider.py`) durchsucht eine Website und holt eine Reihe von Seiten in die Datenbank (`spider.sqlite`), wobei die Links zwischen den Seiten aufgezeichnet werden. Wir können den Prozess jederzeit neu starten, indem wir die Datei `spider.sqlite` entfernen und `spider.py` erneut ausführen.

~~~~
Enter web url or enter: http://www.dr-chuck.com/
['http://www.dr-chuck.com']
How many pages:2
1 http://www.dr-chuck.com/ 12
2 http://www.dr-chuck.com/csev-blog/ 57
How many pages:
~~~~

In diesem Beispieldurchlauf haben wir das Programm angewiesen, eine Website zu crawlen (zu durchforsten) und zwei Seiten abzurufen. Wenn wir das Programm neu starten und ihm sagen, dass es weitere Seiten crawlen soll, wird es keine Seiten, die sich bereits in der Datenbank befinden, erneut crawlen. Beim Neustart wechselt es zu einer zufälligen, nicht gecrawlten Seite und beginnt dort. Also ist jeder nachfolgende Lauf von `spider.py` additiv.

~~~~
Enter web url or enter: http://www.dr-chuck.com/
['http://www.dr-chuck.com']
How many pages:3
3 http://www.dr-chuck.com/csev-blog 57
4 http://www.dr-chuck.com/dr-chuck/resume/speaking.htm 1
5 http://www.dr-chuck.com/dr-chuck/resume/index.htm 13
How many pages:
~~~~

Wir können mehrere Startpunkte in derselben Datenbank haben - im Programm werden diese als „Webs“ bezeichnet. Der Spider wählt nach dem Zufallsprinzip aus allen nicht besuchten Links über alle Webs hinweg die nächste zu durchsuchende Seite aus.

Wenn wir den Inhalt der Datei `spider.sqlite` ausgeben möchten, können wir `spdump.py` wie folgt ausführen:

~~~~
(5, None, 1.0, 3, 'http://www.dr-chuck.com/csev-blog')
(3, None, 1.0, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')
(1, None, 1.0, 2, 'http://www.dr-chuck.com/csev-blog/')
(1, None, 1.0, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')
4 rows.
~~~~

Dies zeigt die Anzahl der eingehenden Links, den alten Page Rank, den neuen Page Rank, die ID der Seite und die URL der Seite. Das Programm `spdump.py` zeigt nur Seiten an, die mindestens einen eingehenden Link auf sie haben.

Sobald wir ein paar Websites in der Datenbank haben, können wir mit dem Programm `sprank.py` das Page-Ranking in Gang setzen. Wir teilen ihm einfach mit, wie viele Page-Rank-Iterationen es ausführen soll.

~~~~
How many iterations:2
1 0.546848992536
2 0.226714939664
[(1, 0.559), (2, 0.659), (3, 0.985), (4, 2.135), (5, 0.659)]
~~~~

Wir können die Datenbank erneut ausgeben, um zu sehen, dass der Seitenrang aktualisiert wurde:

~~~~
(5, 1.0, 0.985, 3, 'http://www.dr-chuck.com/csev-blog')
(3, 1.0, 2.135, 4, 'http://www.dr-chuck.com/dr-chuck/resume/speaking.htm')
(1, 1.0, 0.659, 2, 'http://www.dr-chuck.com/csev-blog/')
(1, 1.0, 0.659, 5, 'http://www.dr-chuck.com/dr-chuck/resume/index.htm')
4 rows.
~~~~

Wir können `sprank.py` so oft ausführen, wie wir möchten und es wird einfach das Page-Ranking jedes Mal verfeinern, wenn wir es ausführen. Wir können `sprank.py` sogar ein paar Mal laufen lassen und dann ein paar weitere Seiten mit `spider.py` durchforsten und dann `sprank.py` laufen lassen, um die Page-Ranking-Werte neu zu konvertieren. Eine Suchmaschine lässt normalerweise sowohl das Crawling- als auch das Ranking-Programm ständig laufen.

Wenn wir die Page-Ranking-Berechnungen neu starten wollen, ohne die Webseiten neu zu laden, können wir `spreset.py` verwenden und dann `sprank.py` neu starten.

~~~~
How many iterations:50
1 0.546848992536
2 0.226714939664
3 0.0659516187242
4 0.0244199333
5 0.0102096489546
6 0.00610244329379
...
42 0.000109076928206
43 9.91987599002e-05
44 9.02151706798e-05
45 8.20451504471e-05
46 7.46150183837e-05
47 6.7857770908e-05
48 6.17124694224e-05
49 5.61236959327e-05
50 5.10410499467e-05
[(512, 0.0296), (1, 12.79), (2, 28.93), (3, 6.808), (4, 13.46)]
~~~~

Für jede Iteration des Page-Ranking-Algorithmus wird die durchschnittliche Änderung des Page-Ranks pro Seite ausgegeben. Das Netzwerk ist anfangs recht unausgewogen, sodass sich die einzelnen Page-Rank-Werte zwischen den Iterationen stark ändern. Aber in ein paar kurzen Iterationen konvergiert der Page-Rank. Wir sollten `sprank.py` lange genug laufen lassen, damit die Page-Ranking-Werte konvergieren.

Wenn wir die aktuellen Top-Seiten in Bezug auf den Page-Rank visualisieren möchten, führen wir `spjson.py` aus, um die Datenbank zu lesen und die Daten für die am stärksten verlinkten Seiten in ein JSON-Format zu schreiben, das in einem Webbrowser angezeigt werden kann.

~~~~
Creating JSON output on spider.json...
How many nodes? 30
Open force.html in a browser to view the visualization
~~~~

Wir können diese Daten betrachten, indem wir die Datei `force.html` in unserem Webbrowser öffnen. Dies zeigt ein automatisches Layout der Knoten und Links. Wir können jeden Knoten anklicken sowie ziehen und wir können auch auf einen Knoten doppelklicken, um die URL zu sehen, die durch den Knoten dargestellt wird.

Wenn wir die anderen Dienstprogramme erneut ausführen, führen wir `spjson.py` ebenfalls erneut aus. Dann drücken wir im Browser auf Aktualisieren, um die neuen Daten aus `spider.json` zu erhalten.

Visualisierung von Mail-Daten
-----------------------------

Bis zu diesem Punkt im Buch sind wir mit unseren Datendateien `mbox-short.txt` und `mbox.txt` ziemlich vertraut geworden. Jetzt ist es an der Zeit, unsere Analyse der E-Mail-Daten auf die nächste Stufe zu bringen.

In der realen Welt müssen wir manchmal Mail-Daten von Servern abrufen. Das kann einige Zeit in Anspruch nehmen und die Daten können inkonsistent und fehlerhaft sein und eine Menge Bereinigungen oder Anpassungen erfordern. In diesem Abschnitt arbeiten wir mit einer Anwendung, die bisher die komplexeste ist, und rufen fast ein Gigabyte an Daten ab und visualisieren sie.

![Eine Wortwolke aus der Sakai-Entwicklerliste](height=3.5in@../images/wordcloud)

Wir können diese Anwendung hier herunterladen:

[https://www.py4e.com/code3/gmane.zip](https://www.py4e.com/code3/gmane.zip)

Wir werden Daten von einem freien E-Mail-Listen-Archivierungsdienst namens [http://www.gmane.org](http://www.gmane.org) verwenden. Dieser Dienst ist sehr beliebt bei Open-Source-Projekten, weil er ein schönes durchsuchbares Archiv der E-Mail-Aktivitäten bietet. Sie haben auch eine sehr liberale Richtlinie bezüglich des Zugriffs auf Daten durch ihre API. Die API hat keine Zugriffsbeschränkungen, wir sollten jedoch darauf achten, den Dienst nicht zu überlasten und nur die Daten zu verwenden, die wir benötigen. Allgemeinen Geschäftsbedingungen von *gmane* sind auf der folgenden Seite zu finden:

[http://www.gmane.org/export.php](http://www.gmane.org/export.php)

*Es ist sehr wichtig, dass wir die Daten von gmane.org verantwortungsvoll nutzen, indem wir unseren Zugriff auf deren Dienste verzögern und lang laufende Aufträge über einen längeren Zeitraum verteilen. Missbrauchen wir diesen kostenlosen Dienst nicht und ruinieren ihn nicht für den Rest von uns.*

Als die Sakai-E-Mail-Daten mit dieser Software gesichtet wurden, produzierte dies fast ein Gigabyte an Daten und erforderte eine Reihe von Durchläufen an mehreren Tagen. Die Datei `README.txt` im obigen ZIP-Archiv enthält möglicherweise Anweisungen, wie wir eine vorab erstellte Kopie der Datei `content.sqlite` für einen Großteil des Sakai-E-Mail-Korpus herunterladen können, damit wir nicht fünf Tage lang „spidern“ müssen, nur um die Programme auszuführen. Wenn wir den vorgespiderten Inhalt herunterladen, sollten wir trotzdem den Spidering-Prozess ausführen, um neuere Nachrichten einzuholen.

Der erste Schritt besteht darin, das gmane-Repository zu durchforsten. Die Basis-URL ist in der `gmane.py` fest codiert und mit der Sakai-Entwicklerliste verknüpft. Wir können ein anderes Repository spidern, indem wir diese Basis-URL ändern. Es ist sicherzustellen, dass die Datei `content.sqlite` gelöscht wird, wenn wir die Basis-URL ändern.

Die Datei `gmane.py` arbeitet als verantwortlicher Caching-Spider, indem sie langsam läuft und eine Mail-Nachricht pro Sekunde abruft, um nicht von gmane gedrosselt zu werden. Das Programm speichert alle Daten in einer Datenbank und kann so oft wie nötig unterbrochen und neu gestartet werden. Es kann viele Stunden dauern, bis alle Daten heruntergeladen sind. Wir müssen also möglicherweise mehrmals neu starten.

Hier ist ein Lauf von `gmane.py`, der die letzten fünf Nachrichten der Sakai-Entwicklerliste abruft:

~~~~
How many messages:10
http://download.gmane.org/gmane.comp.cms.sakai.devel/51410/51411 9460
    nealcaidin@sakaifoundation.org 2013-04-05 re: [building ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51411/51412 3379
    samuelgutierrezjimenez@gmail.com 2013-04-06 re: [building ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51412/51413 9903
    da1@vt.edu 2013-04-05 [building sakai] melete 2.9 oracle ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51413/51414 349265
    m.shedid@elraed-it.com 2013-04-07 [building sakai] ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51414/51415 3481
    samuelgutierrezjimenez@gmail.com 2013-04-07 re: ...
http://download.gmane.org/gmane.comp.cms.sakai.devel/51415/51416 0

Does not start with From
~~~~

Das Programm durchsucht die Datei `content.sqlite` von der ersten bis zur ersten noch nicht gespiderten Nachricht und beginnt das Spidering bei dieser Nachricht. Es fährt mit dem Spidering fort, bis es die gewünschte Anzahl von Nachrichten gespidert hat oder es eine Seite erreicht, die nicht als ordnungsgemäß formatierte Nachricht erscheint.

Manchmal fehlt auf __gmane.org__ eine Nachricht. Vielleicht können Administratoren Nachrichten löschen oder vielleicht gehen sie verloren. Wenn Ihr Spider anhält und es scheint, dass er auf eine fehlende Nachricht gestoßen ist, gehen wir in den SQLite Manager und fügen eine Zeile mit der fehlenden ID hinzu, wobei wir alle anderen Felder leer lassen und `gmane.py` neu starten. Dies wird den Spidering-Prozess wieder freigeben und ihm erlauben, fortzufahren. Diese leeren Nachrichten werden in der nächsten Phase des Prozesses ignoriert.

Eine nette Sache ist, dass wir, sobald wir alle Nachrichten gespidert haben und sie in `content.sqlite` vorliegen, `gmane.py` erneut ausführen können, um neue Nachrichten zu erhalten, sobald sie in die Liste übertragen werden.

Die `content.sqlite`-Daten sind unbearbeitet, mit einem ineffizienten Datenmodell gespeichert, und nicht komprimiert. Dies ist beabsichtigt, da es Ihnen ermöglicht, `content.sqlite` im SQLite-Manager zu betrachten, um Probleme mit dem Spidering-Prozess zu beheben. Es wäre eine schlechte Idee, irgendwelche Abfragen über diese Datenbank laufen zu lassen, da sie ziemlich langsam wären.

Der zweite Vorgang ist die Ausführung des Programms `gmodel.py`. Dieses Programm liest die Rohdaten aus `content.sqlite` und erzeugt eine bereinigte und gut modellierte Version der Daten in der Datei `index.sqlite`. Diese Datei wird viel kleiner sein (oft 10-mal kleiner) als `content.sqlite`, weil auch der Header- und Body-Text komprimiert wird.

Jedes Mal, wenn `gmodel.py` läuft, löscht es die Datei `index.sqlite` und baut sie neu auf, sodass wir die Parameter anpassen und die Mapping-Tabellen in `content.sqlite` bearbeiten können, um den Datenbereinigungsprozess zu optimieren. Dies ist ein Beispiellauf von `gmodel.py`. Jedes Mal, wenn 250 Mail-Nachrichten verarbeitet werden, wird eine Zeile ausgegeben, damit wir den Fortschritt sehen können, denn dieses Programm kann eine Weile laufen und fast ein Gigabyte an Mail-Daten verarbeiten.

~~~~
Loaded allsenders 1588 and mapping 28 dns mapping 1
1 2005-12-08T23:34:30-06:00 ggolden22@mac.com
251 2005-12-22T10:03:20-08:00 tpamsler@ucdavis.edu
501 2006-01-12T11:17:34-05:00 lance@indiana.edu
751 2006-01-24T11:13:28-08:00 vrajgopalan@ucmerced.edu
...
~~~~

Das Programm `gmodel.py` übernimmt eine Reihe von Datenbereinigungsaufgaben.

Domainnamen werden auf zwei Domainlevel für `.com`, `.org`, `.edu` und `.net` gekürzt. Andere Domain-Namen werden auf drei Level gekürzt. So wird `si.umich.edu` zu `umich.edu` und `caret.cam.ac.uk` zu `cam.ac.uk`. E-Mail-Adressen werden ebenfalls in Kleinschreibung überführt, und Adressen wie

~~~~
arwhyte-63aXycvo3TyHXe+LvDLADg@public.gmane.org
~~~~

werden in die echte Adresse umgewandelt, wenn es an anderer Stelle im Nachrichtenkorpus eine passende echte E-Mail-Adresse gibt.

In der Datenbank `mapping.sqlite` gibt es zwei Tabellen, die es Ihnen ermöglichen, sowohl Domänennamen als auch einzelne E-Mail-Adressen zuzuordnen, die sich während der Lebensdauer der E-Mail-Liste ändern. Zum Beispiel hat Steve Githens die folgenden E-Mail-Adressen verwendet, als er während der Lebensdauer der Sakai-Entwicklerliste den Arbeitsplatz wechselte:

~~~~
s-githens@northwestern.edu
sgithens@cam.ac.uk
swgithen@mtu.edu
~~~~

Wir können zwei Einträge in die Mapping-Tabelle in `mapping.sqlite` hinzufügen, sodass `gmodel.py` alle drei auf eine Adresse mappt:

~~~~
s-githens@northwestern.edu ->  swgithen@mtu.edu
sgithens@cam.ac.uk -> swgithen@mtu.edu
~~~~

Wir können auch ähnliche Einträge in der DNSMapping-Tabelle vornehmen, wenn es
mehrere DNS-Namen gibt, die wir einem einzigen DNS zuordnen möchten. Die folgende
Zuordnung wurde zu den Sakai-Daten hinzugefügt:

~~~~
iupui.edu -> indiana.edu
~~~~

damit alle Konten der verschiedenen Standorte der Indiana University
zusammen verfolgt werden.

Wir können die `gmodel.py` immer wieder ausführen, während wir uns die Daten ansehen, und Mappings hinzufügen, um die Daten aufgeräumter zu machen. Wenn wir fertig sind, haben wir eine ordentlich indizierte Version der E-Mail in `index.sqlite`. Dies ist die Datei, die für die Datenanalyse verwendet wird. Mit dieser Datei wird die Datenanalyse wirklich schnell sein.

Die erste und einfachste Datenanalyse besteht darin, festzustellen, wer die meisten Mails verschickt hat und welche Organisation die meisten Mails verschickt hat. Dies wird mit `gbasic.py` durchgeführt:

~~~~
How many to dump? 5
Loaded messages= 51330 subjects= 25033 senders= 1584

Top 5 Email list participants
steve.swinsburg@gmail.com 2657
azeckoski@unicon.net 1742
ieb@tfd.co.uk 1591
csev@umich.edu 1304
david.horwitz@uct.ac.za 1184

Top 5 Email list organizations
gmail.com 7339
umich.edu 6243
uct.ac.za 2451
indiana.edu 2258
unicon.net 2055
~~~~

Es sei zu beachten, wie viel schneller `gbasic.py` im Vergleich zu `gmane.py` oder sogar `gmodel.py` läuft. Wir arbeiten alle mit den gleichen Daten, aber `gbasic.py` verwendet die komprimierten und normalisierten Daten in `index.sqlite`. Wenn wir viele Daten zu verwalten haben, kann ein mehrstufiger Prozess wie in dieser Anwendung etwas länger dauern, aber es kann viel Zeit gespart werden, wenn  Daten untersucht und visualisiert werden.

Eine einfache Visualisierung der Worthäufigkeit in den Betreffzeilen können wir in der Datei `gword.py` erzeugen:

~~~~
Range of counts: 33229 129
Output written to gword.js
~~~~

Dies erzeugt die Datei `gword.js`, die wir mit `gword.htm` visualisieren können, um eine Wortwolke ähnlich der am Anfang dieses Abschnitts zu erzeugen.

Eine zweite Visualisierung wird von `gline.py` erzeugt. Sie berechnet die E-Mail-Beteiligung von Organisationen im Laufe der Zeit.

~~~~
Loaded messages= 51330 subjects= 25033 senders= 1584
Top 10 Oranizations
['gmail.com', 'umich.edu', 'uct.ac.za', 'indiana.edu',
'unicon.net', 'tfd.co.uk', 'berkeley.edu', 'longsight.com',
'stanford.edu', 'ox.ac.uk']
Output written to gline.js
~~~~

Die Ausgabe wird in `gline.js` geschrieben, die mit `gline.htm` visualisiert wird.

![Sakai-Mail-Aktivität pro Organisation](../images/mailorg)

Dies ist eine relativ komplexe und ausgefeilte Anwendung und verfügt über Funktionen zum Abrufen, Bereinigen und Visualisieren von Daten.

